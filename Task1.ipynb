{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "def load_dataset(filepath: str):\n",
    "    \"\"\"Loads a dataset from a given file path.\"\"\"\n",
    "    t = torch.load(filepath)\n",
    "    data, targets = t['data'], t.get('targets')  # Targets may be None for unlabeled data\n",
    "    return data, targets\n",
    "\n",
    "# Step 2: Feature Extraction with MobileNet\n",
    "class MobileNetFeatureExtractor:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        # Load MobileNet without the top classification layer\n",
    "        self.model = tf.keras.applications.MobileNet(\n",
    "            weights='imagenet', include_top=False, pooling='avg', input_shape=input_shape\n",
    "        )\n",
    "\n",
    "    def preprocess(self, data: np.ndarray):\n",
    "        \"\"\"Preprocess raw image data to match MobileNet input requirements.\"\"\"\n",
    "        data = data.astype(np.float32)\n",
    "        resized_data = np.array([tf.image.resize(img, (224, 224)).numpy() for img in data])\n",
    "        preprocessed_data = tf.keras.applications.mobilenet.preprocess_input(resized_data)\n",
    "        return preprocessed_data\n",
    "\n",
    "    def extract(self, data: np.ndarray):\n",
    "        \"\"\"Extract features using MobileNet.\"\"\"\n",
    "        preprocessed_data = self.preprocess(data)\n",
    "        features = self.model.predict(preprocessed_data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "# Step 3: Dimensionality Reduction\n",
    "class FeatureReducer:\n",
    "    def __init__(self, n_components=256):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, data: np.ndarray):\n",
    "        return self.pca.fit_transform(data)\n",
    "\n",
    "    def transform(self, data: np.ndarray):\n",
    "        return self.pca.transform(data)\n",
    "\n",
    "# Step 4: LwP Classifier\n",
    "class LwPClassifier:\n",
    "    def __init__(self, num_classes: int):\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = None\n",
    "\n",
    "    def fit(self, data: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Initialize prototypes based on labeled data.\"\"\"\n",
    "        self.prototypes = []\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_prototype = cls_data.mean(axis=0)\n",
    "                self.prototypes.append(cls_prototype)\n",
    "        self.prototypes = np.array(self.prototypes)\n",
    "\n",
    "    def predict(self, data: np.ndarray):\n",
    "        \"\"\"Predict labels for the given data.\"\"\"\n",
    "        distances = cdist(data, self.prototypes)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update(self, data: np.ndarray, pseudo_labels: np.ndarray, alpha=0.7):\n",
    "        \"\"\"Update prototypes using pseudo-labeled data.\"\"\"\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[pseudo_labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_mean = cls_data.mean(axis=0)\n",
    "                self.prototypes[cls] = alpha * self.prototypes[cls] + (1 - alpha) * cls_mean\n",
    "\n",
    "# Step 5: Training and Evaluation\n",
    "def train_and_evaluate(train_files: list, eval_files: list, num_classes=10, alpha=0.7, confidence_threshold=0.9):\n",
    "    \"\"\"Train models f1, ..., f10 and evaluate on held-out datasets.\"\"\"\n",
    "    accuracies = np.zeros((len(train_files), len(eval_files)))\n",
    "    feature_extractor = MobileNetFeatureExtractor()\n",
    "    feature_reducer = FeatureReducer()\n",
    "    classifier = LwPClassifier(num_classes=num_classes)\n",
    "\n",
    "    # Load and prepare D1\n",
    "    data, targets = load_dataset(train_files[0])\n",
    "    features = feature_extractor.extract(data)\n",
    "    reduced_features = feature_reducer.fit_transform(features)\n",
    "    classifier.fit(reduced_features, np.array(targets))\n",
    "\n",
    "    for i in range(1, len(train_files) + 1):\n",
    "        print(f\"Training model f{i}...\")\n",
    "\n",
    "        # Evaluate on held-out datasets\n",
    "        for j in range(i):  # Evaluate only on \\hat{D}_1 to \\hat{D}_i\n",
    "            eval_data, eval_targets = load_dataset(eval_files[j])\n",
    "            eval_features = feature_extractor.extract(eval_data)\n",
    "            eval_reduced_features = feature_reducer.transform(eval_features)\n",
    "            predictions = classifier.predict(eval_reduced_features)\n",
    "            accuracies[i - 1, j] = accuracy_score(eval_targets, predictions)\n",
    "\n",
    "        # Stop after f10\n",
    "        if i == len(train_files):\n",
    "            break\n",
    "\n",
    "        # Load next unlabeled dataset (D2, ..., D10)\n",
    "        next_data, _ = load_dataset(train_files[i])\n",
    "        next_features = feature_extractor.extract(next_data)\n",
    "        next_reduced_features = feature_reducer.transform(next_features)\n",
    "\n",
    "        # Predict labels for next dataset\n",
    "        pseudo_labels = classifier.predict(next_reduced_features)\n",
    "\n",
    "        # Confidence filtering\n",
    "        if confidence_threshold:\n",
    "            distances = cdist(next_reduced_features, classifier.prototypes)\n",
    "            confidence = 1 - (distances.min(axis=1) / distances.max(axis=1))\n",
    "            mask = confidence >= confidence_threshold\n",
    "            next_reduced_features = next_reduced_features[mask]\n",
    "            pseudo_labels = pseudo_labels[mask]\n",
    "\n",
    "        # Update classifier using pseudo-labeled data\n",
    "        classifier.update(next_reduced_features, pseudo_labels, alpha=alpha)\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "# Example usage\n",
    "train_files = [f\"part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]  # Replace with actual paths\n",
    "eval_files = [f\"part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]  # Replace with actual paths\n",
    "accuracies = train_and_evaluate(train_files, eval_files)\n",
    "print(\"Accuracy matrix:\")\n",
    "print(accuracies)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
